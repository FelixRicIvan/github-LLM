Step 	Training Loss
1 	5.946200
2 	4.439600
3 	3.650400
4 	3.235000
5 	3.192400
6 	2.656800
7 	3.057600
8 	2.289900
9 	2.650800
10 	1.974100
11 	2.115800
12 	2.330700
13 	2.034900
14 	2.314900
15 	2.438600
16 	2.309000
17 	2.619800
18 	2.036200
19 	2.215300
20 	1.853700
21 	1.700300
22 	2.070600
23 	1.546400
24 	2.129200
25 	1.725700
26 	2.043800
27 	2.165700
28 	2.400600
29 	2.207500
30 	2.409200
31 	2.254600
32 	2.068900
33 	1.496700
34 	2.040800
35 	1.924700
36 	1.995700
37 	2.018700
38 	2.310000
39 	2.092000
40 	2.162700
41 	2.115000
42 	2.088200
43 	2.201600
44 	1.893300
45 	2.242700
46 	1.996300
47 	1.947900
48 	1.969900
49 	1.770000
50 	1.959100
51 	1.758300
52 	2.209500
53 	1.963600
54 	1.918400
55 	1.935400
56 	1.847600
57 	1.880900
58 	2.076600
59 	1.974200
60 	2.232100
61 	2.120300
62 	1.616500
63 	2.000400
64 	1.872000
65 	1.691000
66 	1.635200
67 	1.925400
68 	2.115400
69 	1.817200
70 	1.731900
71 	1.475500
72 	1.766800
73 	1.963800
74 	1.957000
75 	2.049200
76 	1.994200
77 	1.539700
78 	1.681800
79 	1.850000
80 	1.937400
81 	1.603300
82 	1.844000
83 	1.676800
84 	1.928600
85 	2.005300
86 	2.091800
87 	1.948800
88 	1.834900
89 	2.307100
90 	1.724900
91 	1.648900
92 	1.425500
93 	1.613100
94 	2.207600
95 	1.612700
96 	2.159000
97 	1.386200
98 	1.816100
99 	1.540800
100 	1.792100

TrainOutput(global_step=100, training_loss=2.0901457619667054, metrics={'train_runtime': 715.2264, 'train_samples_per_second': 1.119, 'train_steps_per_second': 0.14, 'total_flos': 1199122999262208.0, 'train_loss': 2.0901457619667054})

('merged_model/tokenizer_config.json',
 'merged_model/special_tokens_map.json',
 'merged_model/tokenizer.json')
